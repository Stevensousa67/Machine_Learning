{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0pmyE9e2n3Q"
   },
   "source": [
    "## Introduction to Assignment\n",
    "\n",
    "\n",
    "In this section of the assignment, you will create machine learning models using K Nearest Neighbors and Decision Trees for Fashion MNIST dataset. Specific instruction for that part of the problem can be found in the corresponding cells above the code. \n",
    "\n",
    "Note, you only know the labels of the training dataset. The labels of the test dataset are hidden from you. You will perfrom model selection with cross-validation on the training set.  \n",
    "\n",
    "After obtaining parameters, upload you submissions to Blackboard. You can try and fine tune your parameters to obtain a high performance.\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. You can directly run this notebook on Google colab if your machine is slower. Upload the data and get started!\n",
    "\n",
    "2. Go through the documentation of sklearn carefully.\n",
    "\n",
    "3. Don't try to run the code for all 60,000 data points. Rather first try to verify implementation using 10,000 data points, scale it up to 60,000. If you don't do this, you'll spend lot more time debugging between each iteration. Make the code work first. \n",
    "\n",
    "4. Complete all the classifiers before trying to optimize.\n",
    "\n",
    "5. For cross-validation, you can use gridsearchcv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "sOOuDTZv0tfM",
    "outputId": "8ee26e0c-1a53-4a6e-a278-564ee7a806e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for uploading the csv files to Google Colab. \n",
    "# Skip this step if you are using Jupyter installed on your computer. In that case, just put CSV files in the same folder \n",
    "# as your notebook.\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "kWF3HSdSCEFg",
    "outputId": "d3a02302-79d0-48cb-9443-78f2baf5ae8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 784)\n",
      "y_train shape : (60000,)\n",
      "x_test shape:  (10000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "## Code to load data from train and test csv(s)\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train = train.iloc[:,2:].to_numpy()\n",
    "y_train = train.iloc[:,:1].to_numpy()\n",
    "\n",
    "m,n = y_train.shape\n",
    "\n",
    "y_train = y_train.reshape(m)\n",
    "\n",
    "X_test = test.iloc[:,1:].to_numpy()\n",
    "\n",
    "\n",
    "def showImage(data):\n",
    "    some_article = data\n",
    "    some_article_image = some_article.reshape(28, 28) # Reshaping it to get the 28x28 pixels\n",
    "    plt.imshow(some_article_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('x_train shape: ', X_train.shape)\n",
    "print('y_train shape :', y_train.shape)\n",
    "\n",
    "print('x_test shape: ', X_test.shape)\n",
    "\n",
    "\n",
    "showImage(X_train[1])\n",
    "print(y_train[1])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTu7-SSWvTCd"
   },
   "source": [
    "## KNN Classifier\n",
    "\n",
    "Implement a KNN classifier with 5-fold cross validation. What is the best value of n that you obtained? What happens if you increase value of n more than your best value? Use {3, 5, 7, 9, 11} values for n. \n",
    "\n",
    "\n",
    "What is the time complexity of the KNN algorithm with naive search approach? How can you improve upon the naive search to reduce the time complexity? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier - Answers\n",
    "\n",
    "### Results\n",
    "- **Best accuracy**: 0.8186\n",
    "- **Best value of n**: 5\n",
    "\n",
    "### Accuracy for each n:\n",
    "- n=3, Accuracy=0.8156\n",
    "- n=5, Accuracy=0.8186\n",
    "- n=7, Accuracy=0.8162\n",
    "- n=9, Accuracy=0.8131\n",
    "- n=11, Accuracy=0.8166\n",
    "\n",
    "### Answers\n",
    "1. **What is the best value of n that you obtained?**  \n",
    "   The best value of `n` is 5, yielding an accuracy of 0.8186 with 5-fold cross-validation on 10,000 samples.\n",
    "\n",
    "2. **What happens if you increase the value of n more than your best value?**  \n",
    "   Increasing `n` beyond 5 reduces accuracy slightly at first, then shows a minor recovery: `n=7` (0.8162), `n=9` (0.8131), and `n=11` (0.8166), all below 0.8186. Larger `n` includes more distant neighbors, diluting the vote and over-smoothing decision boundaries, though `n=11`’s slight rise indicates some stability.\n",
    "\n",
    "3. **What is the time complexity of the KNN algorithm with naive search approach?**  \n",
    "   - Training: **O(1)** (just stores data).  \n",
    "   - Prediction: **O(m_test * m * d)**, where `m=10,000`, `d=784`, and `m_test` is the test set size. Distance computation dominates.\n",
    "\n",
    "4. **How can you improve upon the naive search to reduce the time complexity?**  \n",
    "   Use a KD-Tree, reducing prediction to **O(m_test * log m)** (though high `d=784` lessens efficiency). Alternatively, PCA can reduce `d`, cutting distance costs to **O(m * d_reduced)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssqNvMSuJTQ2",
    "outputId": "553e1b24-7b85-4410-c457-fb2bcf86e509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.8186\n",
      "Best n: 5\n",
      "\n",
      "Accuracy for each n:\n",
      "n=3, Accuracy=0.8156\n",
      "n=5, Accuracy=0.8186\n",
      "n=7, Accuracy=0.8162\n",
      "n=9, Accuracy=0.8131\n",
      "n=11, Accuracy=0.8166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# taking 10000 samples from the training sets\n",
    "X_train_sample = X_train_scaled[:10000]\n",
    "y_train_sample = y_train[:10000]\n",
    "\n",
    "# the n values\n",
    "n_values = [3, 5, 7, 9, 11]\n",
    "\n",
    "# putting the n values as the parameter for cross validation\n",
    "param_grid = [{'n_neighbors': n_values}]\n",
    "\n",
    "# performing cross validation on the KNN classifier using the given n values\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# fitting a model to the data\n",
    "grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# showing the best accuracy\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# showing the parameter that leads to the best accuracy\n",
    "best_n = grid_search.best_params_['n_neighbors']\n",
    "print(f\"Best n: {best_n}\")\n",
    "\n",
    "# Printing accuracy for each n\n",
    "print(\"\\nAccuracy for each n:\")\n",
    "for n, score in zip(n_values, grid_search.cv_results_['mean_test_score']):\n",
    "    print(f\"n={n}, Accuracy={score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZOiEFN6dVFu"
   },
   "source": [
    "## Decision Tree Classifier 1\n",
    "\n",
    " Train five different decision trees. Use the following max depths (10, 11, 12, 13, 14) How does the maximum depth of the tree affect the estimated accuracy? Explain in at most 4 sentences. Choose the model with lowest estimated out of sample error, train it with the full training set, and predict the labels for the images in the test set using code given at the end of the notebook. Make sure that your report clearly states which model was chosen and why.\n",
    "\n",
    " What does default value ccp_alpha=0.0 signify for the decision tree classifier?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier 1 - Answers\n",
    "\n",
    "### Results\n",
    "- **Best accuracy**: 0.7684\n",
    "- **Best max_depth**: 11\n",
    "- **Accuracy for each max_depth**:  \n",
    "  - max_depth=10, Accuracy=0.7678  \n",
    "  - max_depth=11, Accuracy=0.7684  \n",
    "  - max_depth=12, Accuracy=0.7664  \n",
    "  - max_depth=13, Accuracy=0.7617  \n",
    "  - max_depth=14, Accuracy=0.7621  \n",
    "- **Predicted labels (first 10)**: [6, 6, 6, 2, 6, 6, 2, 6, 0, 6]\n",
    "\n",
    "### Answers\n",
    "1. **How does the maximum depth of the tree affect the estimated accuracy?**  \n",
    "   At `max_depth=11`, the tree achieves the highest accuracy (0.7684), effectively capturing the data’s patterns. Increasing depth to 12 and beyond reduces accuracy (e.g., 0.7664 at 12, 0.7617 at 13, and 0.7621 at 14), indicating overfitting as the tree models noise. A slight uptick at 14 (0.7621) doesn’t recover to 11’s level, suggesting deeper trees overcomplicate the model. Thus, `max_depth=11` is the optimal depth, balancing fit and generalization.\n",
    "\n",
    "2. **Chosen Model and Prediction**:  \n",
    "   The model with `max_depth=11` was chosen, achieving the highest accuracy (0.7684) and lowest out-of-sample error (1 - 0.7684 = 0.2316) in 5-fold cross-validation on 10,000 samples. It was selected for its superior generalization over depths 12, 13, and 14. This model was trained on the full dataset (`X_train_scaled`, `y_train`) and used to predict `X_test` labels, saving them to `prediction.csv` with the first 10 samples as [6, 6, 6, 2, 6, 6, 2, 6, 0, 6].\n",
    "\n",
    "3. **What does default value ccp_alpha=0.0 signify?**  \n",
    "   `ccp_alpha=0.0` means no pruning occurs, allowing the tree to grow fully up to `max_depth=11` without complexity penalties. This can risk overfitting, especially if depth increases, as no branches are trimmed based on impurity gains. A positive `ccp_alpha` would prune less significant splits to simplify the tree and improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be_mkXSW5dnm",
    "outputId": "2eb8ef79-6446-4771-ec1a-1a2f08f04600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.7684\n",
      "Best max depth: 11\n",
      "\n",
      "Accuracy for each max depth:\n",
      "Max depth=10, Accuracy=0.7678\n",
      "Max depth=11, Accuracy=0.7684\n",
      "Max depth=12, Accuracy=0.7664\n",
      "Max depth=13, Accuracy=0.7617\n",
      "Max depth=14, Accuracy=0.7621\n",
      "\n",
      "Predicted labels for the test set (first 10 samples):\n",
      "[6 6 6 2 6 6 2 6 0 6]\n"
     ]
    }
   ],
   "source": [
    "# importing the tree library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# the max depth values\n",
    "max_depths = [10, 11, 12, 13, 14]\n",
    "\n",
    "# putting the depths as the parameter\n",
    "param_grid = [{'max_depth': max_depths}]\n",
    "\n",
    "# performing 5-fold cross validation on our decision tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# fitting a model to our data\n",
    "grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# showing the best accuracy for the model\n",
    "print(f\"Best accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# showing the parameter that gives the best accuracy\n",
    "print(f\"Best max depth: {grid_search.best_params_['max_depth']}\")\n",
    "\n",
    "# Printing accuracy for each max depth\n",
    "print(\"\\nAccuracy for each max depth:\")\n",
    "for depth, score in zip(max_depths, grid_search.cv_results_['mean_test_score']):\n",
    "    print(f\"Max depth={depth}, Accuracy={score:.4f}\")\n",
    "\n",
    "# using the best value for parameter to train a model on the entire dataset\n",
    "best_dt_model = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth'])\n",
    "best_dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting labels for the test set\n",
    "y_test_pred = best_dt_model.predict(X_test)\n",
    "print(\"\\nPredicted labels for the test set (first 10 samples):\")\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmokAZyRaZPR"
   },
   "source": [
    "## Decision Tree Classifier 2\n",
    "\n",
    "Train five different decision trees using five-fold cross validation. Use the following values for ccp_alpha (0.00005,0.0005,0.005,0.05). How does the ccp_alpha of the tree affect the estimated accuracy? Explain in at most 4 sentences. Keep all the other parameters to default value. Choose the model with lowest estimated out of sample error, train it with the full training set, and predict the labels for the images in the test set. Finally using prediction code given at the end of the notebook generate predictions. \n",
    "\n",
    "Compare the best tree obtained for max-depth, with the best tree classifier obtained for ccp_alpha. Is there a difference in their errors? Why?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier 2 - Answers\n",
    "\n",
    "### Results\n",
    "- **Best accuracy**: 0.7737\n",
    "- **Best ccp_alpha**: 0.0005\n",
    "- **Accuracy for each ccp_alpha**:  \n",
    "  - ccp_alpha=0.00005, Accuracy=0.7530  \n",
    "  - ccp_alpha=0.0005, Accuracy=0.7737  \n",
    "  - ccp_alpha=0.005, Accuracy=0.7221  \n",
    "  - ccp_alpha=0.05, Accuracy=0.3625  \n",
    "- **Predicted labels (first 10)**: [6, 6, 6, 6, 6, 6, 6, 6, 8, 6]\n",
    "\n",
    "### Answers\n",
    "1. **How does the ccp_alpha of the tree affect the estimated accuracy?**  \n",
    "   A very small `ccp_alpha=0.00005` yields moderate accuracy (0.7530) with minimal pruning, while `ccp_alpha=0.0005` increases accuracy to 0.7737 by optimally trimming less impactful splits. At `ccp_alpha=0.005`, accuracy drops to 0.7221 as more pruning removes useful branches, and `ccp_alpha=0.05` drastically reduces it to 0.3625 due to excessive simplification. Larger `ccp_alpha` values over-prune, causing underfitting, while smaller ones risk overfitting. Thus, `ccp_alpha=0.0005` best balances complexity and generalization.\n",
    "\n",
    "2. **Chosen Model and Prediction**:  \n",
    "   The model with `ccp_alpha=0.0005` was chosen, achieving the highest accuracy (0.7737) and lowest out-of-sample error (1 - 0.7737 = 0.2263) in 5-fold cross-validation on 10,000 samples. It was selected for its superior generalization over other alpha values. This model was trained on the full dataset (`X_train_scaled`, `y_train`) and used to predict `X_test` labels, saving them to `prediction2.csv` with the first 10 samples as [6, 6, 6, 6, 6, 6, 6, 6, 8, 6].\n",
    "\n",
    "3. **Comparison with Best Max-Depth Tree**:  \n",
    "   The best `max_depth=11` tree from Classifier 1 had an accuracy of 0.7684 (error = 0.2316), while the best `ccp_alpha=0.0005` tree here has a higher accuracy of 0.7737 (error = 0.2263), a difference of 0.0053. This indicates `ccp_alpha` pruning slightly outperforms `max_depth` limiting. The difference arises because `ccp_alpha` prunes based on cost-complexity, retaining impactful splits regardless of depth, whereas `max_depth=11` imposes a strict limit, potentially discarding useful deeper splits. The modest improvement suggests both methods control overfitting effectively, but `ccp_alpha` offers more precise tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8DaZiaP4H6I",
    "outputId": "6a33243a-b38f-44fa-bfe9-37e1ab8aea65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.7737\n",
      "Best ccp_alpha: 0.0005\n",
      "\n",
      "Accuracy for each ccp_alpha:\n",
      "ccp_alpha=5e-05, Accuracy=0.7530\n",
      "ccp_alpha=0.0005, Accuracy=0.7737\n",
      "ccp_alpha=0.005, Accuracy=0.7221\n",
      "ccp_alpha=0.05, Accuracy=0.3625\n",
      "\n",
      "Predicted labels for the test set (first 10 samples):\n",
      "[6 6 6 6 6 6 6 6 8 6]\n"
     ]
    }
   ],
   "source": [
    "# The given alpha values\n",
    "alphas = [0.00005, 0.0005, 0.005, 0.05]\n",
    "\n",
    "# Putting the alpha values as parameters\n",
    "param_grid = [{'ccp_alpha': alphas}]\n",
    "\n",
    "# Performing 5-fold cross validation on the decision tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fitting a model on the sample data\n",
    "grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Printing the best accuracy\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Printing the parameter that results in the best value\n",
    "best_alpha = grid_search.best_params_['ccp_alpha']\n",
    "print(f\"Best ccp_alpha: {best_alpha}\")\n",
    "\n",
    "# Printing accuracy for each alpha\n",
    "print(\"\\nAccuracy for each ccp_alpha:\")\n",
    "for alpha, score in zip(alphas, grid_search.cv_results_['mean_test_score']):\n",
    "    print(f\"ccp_alpha={alpha}, Accuracy={score:.4f}\")\n",
    "\n",
    "# Using the best parameter value to train a model on the whole dataset\n",
    "best_dt_model = DecisionTreeClassifier(ccp_alpha=grid_search.best_params_['ccp_alpha'], random_state=42)\n",
    "best_dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting labels for the test set\n",
    "y_test_pred = best_dt_model.predict(X_test)\n",
    "print(\"\\nPredicted labels for the test set (first 10 samples):\")\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1CQeXO4dbK8"
   },
   "source": [
    "## Prediction code \n",
    "\n",
    "use this code to generate prediction.csv for you classifier. Replace xgb_clf by corresponding classifier to obtain your prediction.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJyZcVtjU0mt"
   },
   "outputs": [],
   "source": [
    "## code to generate predictions\n",
    "\n",
    "import csv\n",
    "\n",
    "# Generating predictions for the test set and saving to prediction.csv\n",
    "predictions = np.zeros(10000, dtype=int)\n",
    "for i in range(10000):\n",
    "    predictions[i] = best_dt_model.predict(X_test[i].reshape(1, -1))[0]  # Extract the single value\n",
    "\n",
    "# Save predictions to CSV with id and label columns (fixing ValueError)\n",
    "prediction_df = pd.DataFrame({\n",
    "    'id': np.arange(10000),  # Add id column from 0 to 9999\n",
    "    'label': predictions\n",
    "})\n",
    "prediction_df.to_csv('prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
